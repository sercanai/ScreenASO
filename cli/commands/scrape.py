"""Scrape commands for Screen ASO."""

from __future__ import annotations

from datetime import datetime, timezone
from pathlib import Path

import typer
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn

from cli.utils.output import OutputManager
from cli.utils.scraping import scrape_app_data
from cli.utils.validation import ValidationError, Validator
from core.privacy import sanitize_reviews_for_output

scrape_app = typer.Typer(help="ðŸ“± Scrape app data and reviews from both stores")
console = Console()
output_manager = OutputManager()
validator = Validator()


@scrape_app.command()
def app(
    app_id: str = typer.Argument(..., help="App ID (numeric for App Store: 123456, or package for Play Store: com.example.app)"),
    reviews: int = typer.Option(
        50, "--reviews", "-r", help="Number of reviews to scrape (default: 50, max: 1000)"
    ),
    country: str = typer.Option("US", "--country", "-c", help="Country code (US, TR, GB, DE, etc.)"),
) -> None:
    """Scrape a single app's data and reviews.

    Store is automatically detected based on app ID format:
      â€¢ Numeric ID (123456) â†’ Apple App Store
      â€¢ Package name (com.example.app) â†’ Google Play Store

    Examples:
      aso-cli scrape app 123456789 --reviews 100
      aso-cli scrape app com.instagram.android --country TR
      aso-cli scrape app 1495297747 --reviews 500 --country US
    """
    try:
        # Validate inputs
        app_id = validator.validate_app_id(app_id)
        reviews = validator.validate_limit(reviews)
        country = validator.validate_country_code(country)

        console.print(f"Scraping app {app_id} in {country} (reviews: {reviews})")

        store_label, result = scrape_app_data(app_id, country=country, reviews=reviews)
        result.setdefault("store", store_label)
        console.print(f"[blue]Detected {store_label} app[/blue]")

        fetched_at = datetime.now(timezone.utc).isoformat()
        store_slug = output_manager.infer_store_slug(store=store_label)
        payload = {
            "query": {
                "app_id": app_id,
                "country": country,
                "reviews": reviews,
                "store": store_label,
            },
            "fetched_at": fetched_at,
            "app": result,
        }

        # Save results
        app_slug = output_manager.derive_app_slug(app=result, store=store_slug)
        filename = output_manager.get_timestamped_filename("scrape")
        output_path = output_manager.save_json(
            sanitize_reviews_for_output(payload),
            "scrapes",
            filename,
            store=store_slug,
            slug=app_slug,
            context="scrape",
        )

        # Display summary
        console.print(f"[green]âœ“[/green] Scraped app {app_id}")
        output_manager.print_summary(
            {
                "App ID": app_id,
                "Store": store_label,
                "Country": country,
                "Reviews Requested": reviews,
                "Reviews Found": len(result.get("reviews", [])),
                "Output": str(output_path),
            }
        )

    except ValidationError as e:
        console.print(f"[red]Validation error: {e}[/red]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@scrape_app.command()
def batch(
    input: str = typer.Argument(..., help="Input JSON file with app list (from search command)"),
    reviews_limit: int = typer.Option(
        50, "--reviews-limit", "-r", help="Reviews limit per app (default: 50)"
    ),
    country: str = typer.Option("US", "--country", "-c", help="Country code"),
) -> None:
    """Scrape multiple apps from input file.

    The input file should be generated by the search command and contain
    an 'apps' array with app_id fields.

    Examples:
      aso-cli search app-store "fitness" --limit 5 --output search_results.json
      aso-cli scrape batch search_results.json --reviews-limit 100
      aso-cli scrape batch outputs/searches/*.json --country TR
    """
    try:
        # Validate inputs
        reviews_limit = validator.validate_limit(reviews_limit)
        input_file = validator.validate_file_exists(input)
        country = validator.validate_country_code(country)

        console.print(f"Batch scraping from {input} (reviews limit: {reviews_limit})")

        # Load input file
        data = output_manager.load_json(input_file)
        source_keyword = (
            data.get("query", {}).get("keyword") if isinstance(data, dict) else None
        )
        batch_slug = output_manager.slugify(
            source_keyword or Path(input).stem or "batch"
        )
        apps = data.get("apps", [])

        if not apps:
            console.print("[yellow]No apps found in input file[/yellow]")
            return

        console.print(f"Found {len(apps)} apps to scrape")

        # Scrape each app
        results = []
        stores_found: set[str] = set()
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task(f"Scraping {len(apps)} apps...", total=len(apps))

            for app_data in apps:
                raw_app_id = app_data.get("app_id") or app_data.get("id")
                app_name = app_data.get("app_name") or app_data.get("name", "Unknown")

                if not raw_app_id:
                    progress.console.print(
                        f"[yellow]Skipping app without ID: {app_name}[/yellow]"
                    )
                    progress.advance(task)
                    continue

                app_id = str(raw_app_id)
                progress.console.print(f"Scraping: {app_name} ({app_id})")

                try:
                    store_label, result = scrape_app_data(
                        app_id, country=country, reviews=reviews_limit
                    )
                    result.setdefault("store", store_label)
                    store_slug = output_manager.infer_store_slug(store=store_label)
                    if store_slug:
                        stores_found.add(store_slug)
                    progress.console.print(
                        f"[green]âœ“[/green] Scraped: {app_name} [{store_label}]"
                    )
                    results.append(result)
                    single_payload = {
                        "query": {
                            "app_id": app_id,
                            "country": country,
                            "reviews": reviews_limit,
                            "store": store_label,
                        },
                        "fetched_at": datetime.now(timezone.utc).isoformat(),
                        "app": result,
                    }
                    single_slug = output_manager.derive_app_slug(
                        app=result, store=store_slug
                    )
                    single_filename = output_manager.get_timestamped_filename("scrape")
                    output_manager.save_json(
                        sanitize_reviews_for_output(single_payload),
                        "scrapes",
                        single_filename,
                        store=store_slug,
                        slug=single_slug,
                        context="scrape",
                    )
                except Exception as e:
                    progress.console.print(
                        f"[red]âœ—[/red] Failed to scrape {app_name}: {e}"
                    )

                progress.advance(task)

        # Save results
        batch_store_slug = (
            next(iter(stores_found))
            if len(stores_found) == 1
            else ("multi-store" if stores_found else None)
        )
        filename = output_manager.get_timestamped_filename("batch_scrape")
        batch_payload = {
            "query": {
                "input_file": input,
                "reviews_limit": reviews_limit,
                "country": country,
            },
            "fetched_at": datetime.now(timezone.utc).isoformat(),
            "apps": results,
        }
        output_path = output_manager.save_json(
            sanitize_reviews_for_output(batch_payload),
            "scrapes",
            filename,
            store=batch_store_slug,
            slug=batch_slug,
            context="batch-scrape",
        )

        # Display summary
        console.print(f"[green]âœ“[/green] Batch scraping completed")
        output_manager.print_summary(
            {
                "Total Apps": len(apps),
                "Successfully Scraped": len(results),
                "Failed": len(apps) - len(results),
                "Output": str(output_path),
            }
        )

    except ValidationError as e:
        console.print(f"[red]Validation error: {e}[/red]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)


@scrape_app.command()
def enrich(
    input: str = typer.Argument(..., help="Input JSON file to enrich (from scrape command)"),
) -> None:
    """Enrich existing app data with sentiment analysis and additional metadata.

    Adds sentiment analysis, keyword extraction, and review classification
    to previously scraped app data.

    Examples:
      aso-cli scrape enrich outputs/scrapes/app_123456.json
      aso-cli scrape enrich batch_scrape_20241201_120000.json
    """
    try:
        # Validate inputs
        input_file = validator.validate_file_exists(input)

        console.print(f"Enriching data from {input}")

        # Load input file
        data = output_manager.load_json(input_file)

        # Import sentiment analyzer
        from core.sentiment.pipeline import SentimentAnalyzer

        analyzer = SentimentAnalyzer()

        # Process apps
        apps = data.get("apps", [])
        if not apps:
            console.print("[yellow]No apps found in input file[/yellow]")
            return

        stores_found: set[str] = set()
        enriched_slug = output_manager.slugify(Path(input).stem or "enriched")
        console.print(f"Enriching {len(apps)} apps with sentiment analysis...")

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            task = progress.add_task("Enriching apps...", total=len(apps))

            for app in apps:
                reviews = app.get("reviews", [])
                if reviews:
                    # Add sentiment analysis to each review
                    for review in reviews:
                        if "review_text" in review or "text" in review:
                            text = review.get("review_text") or review.get("text", "")
                            if text:
                                sentiment = analyzer.analyze_text(text)
                                review["sentiment"] = sentiment

                    # Add aggregate sentiment stats
                    sentiments = [
                        r.get("sentiment", {}).get("label")
                        for r in reviews
                        if r.get("sentiment")
                    ]
                    if sentiments:
                        app["sentiment_summary"] = {
                            "positive": sentiments.count("positive"),
                            "negative": sentiments.count("negative"),
                            "neutral": sentiments.count("neutral"),
                            "total": len(sentiments),
                        }

                store_slug = output_manager.infer_store_slug(app=app)
                if store_slug:
                    stores_found.add(store_slug)
                progress.advance(task)

        # Save enriched data
        enriched_store = (
            next(iter(stores_found))
            if len(stores_found) == 1
            else ("multi-store" if stores_found else None)
        )
        enriched_at = datetime.now(timezone.utc).isoformat()
        filename = output_manager.get_timestamped_filename("enriched")
        enriched_payload = {
            "source_file": input,
            "enriched_at": enriched_at,
            "apps": apps,
        }
        output_path = output_manager.save_json(
            sanitize_reviews_for_output(enriched_payload),
            "scrapes",
            filename,
            store=enriched_store,
            slug=enriched_slug,
            context="enriched",
        )
        for app in apps:
            store_slug = output_manager.infer_store_slug(app=app)
            slug = output_manager.derive_app_slug(app=app, store=store_slug)
            single_payload = {
                "source_file": input,
                "enriched_at": enriched_at,
                "app": app,
            }
            single_filename = output_manager.get_timestamped_filename("enriched")
            output_manager.save_json(
                sanitize_reviews_for_output(single_payload),
                "scrapes",
                single_filename,
                store=store_slug,
                slug=slug,
                context="enriched",
            )

        console.print(f"[green]âœ“[/green] Data enriched successfully")
        output_manager.print_summary(
            {
                "Total Apps": len(apps),
                "Total Reviews Analyzed": sum(
                    len(app.get("reviews", [])) for app in apps
                ),
                "Output": str(output_path),
            }
        )

    except ValidationError as e:
        console.print(f"[red]Validation error: {e}[/red]")
        raise typer.Exit(1)
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)
